[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = /opt/airflow/dags

# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
default_timezone = Europe/Paris

# The executor class that airflow should use. Since you are using CeleryExecutor
executor = CeleryExecutor

# Configures the maximum number of task instances that can run concurrently per scheduler
parallelism = 32

# Max number of active tasks allowed to run concurrently in each DAG
max_active_tasks_per_dag = 16

# The maximum number of active DAG runs per DAG.
max_active_runs_per_dag = 16

# Default timezone for displaying dates in the UI
default_ui_timezone = Europe/Paris

# Enable XCom Pickling for task instance communication
enable_xcom_pickling = False

# Set the fernet_key for encrypted connections
fernet_key = 

[database]
# The SQLAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engines.
# Airflow metadata will be stored in MySQL (configured in the docker-compose file)
sql_alchemy_conn = mysql+mysqldb://admin_dataguy:xxxxx@mysql/airflow_metadata

# Extra engine specific keyword args passed to SQLAlchemy's create_engine, as a JSON-encoded value
sql_alchemy_engine_args = {}

# The encoding for the databases
sql_engine_encoding = utf-8

# If SqlAlchemy should pool database connections
sql_alchemy_pool_enabled = True

# The SqlAlchemy pool size is the maximum number of database connections
sql_alchemy_pool_size = 5

# The SqlAlchemy pool recycle is the number of seconds a connection can be idle before it is invalidated
sql_alchemy_pool_recycle = 1800

# Enable connection checks before checkout from the connection pool
sql_alchemy_pool_pre_ping = True

[celery]
# Configurations for CeleryExecutor (used for task execution)
broker_url = redis://redis:6379/0  # Redis broker URL for Celery
result_backend = db+mysql://admin_dataguy:xxxxx@mysql/airflow_metadata  # Celery task result backend

worker_concurrency = 16

# Celery Flower UI configurations
flower_host = 0.0.0.0
flower_port = 5555

# Whether Celery should use remote control to manage workers
worker_enable_remote_control = True

# The Celery result backend uses MySQL to store task results
result_backend_sqlalchemy_engine_options = {}

[scheduler]
# Controls how often the scheduler will run to trigger new tasks (in seconds)
scheduler_heartbeat_sec = 5

# The maximum time to wait before deactivating stale DAGs
stale_dag_threshold = 50

# Whether the scheduler should do catchup for DAGs
catchup_by_default = True

[webserver]
# Webserver configurations for the Airflow UI
web_server_host = 0.0.0.0
web_server_port = 8080
base_url = http://localhost:8080
workers = 4
worker_class = sync
authenticate = True                          # Enable authentication
auth_backend = airflow.api.auth.backend.default  # Set the default authentication backend

# Security configurations for the web server
session_backend = database
secret_key = B/le++OOrAIsv3RAmga6PA==
cookie_secure = False
cookie_samesite = Lax

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow logging level
logging_level = INFO

# Whether to use colored logs in the console
colored_console_log = True

# Format of the log line
log_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s

[metrics]
# StatsD integration for metrics
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[celery_kubernetes_executor]
# This section is used if you plan to use CeleryExecutor with KubernetesExecutor.
kubernetes_queue = kubernetes

[sensors]
# Default timeout for sensors, set for 7 days
default_timeout = 604800

[api]
# Configuration for the API client
api_client = airflow.api.client.local_client
endpoint_url = http://localhost:8080

# Enable the deprecated experimental API
enable_experimental_api = False

[triggerer]
# Configuration for the triggerer to manage tasks
default_capacity = 1000
job_heartbeat_sec = 5